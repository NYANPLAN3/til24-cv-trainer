{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Build a YOLOv9c model from scratch\n",
    "# model = YOLO(\"yolov9c.yaml\")\n",
    "\n",
    "# Build a YOLOv9c model from pretrained weight\n",
    "model = YOLO(\"yolov9c.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with the YOLOv9c model on the 'bus.jpg' image\n",
    "results = model(\"../bus.jpg\")\n",
    "for result in results:\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspath = \"/workspaces/til24-cv-trainer/data/til24ufo/dataset.yaml\"\n",
    "results = model.train(\n",
    "    data=dspath,\n",
    "    epochs=80,\n",
    "    patience=10,\n",
    "    batch=16,\n",
    "    imgsz=1440,\n",
    "    save_period=1,\n",
    "    cache=\"ram\",\n",
    "    device=0,\n",
    "    workers=12,\n",
    "    freeze=20,\n",
    "    seed=42,\n",
    "    deterministic=False,\n",
    "    profile=True,\n",
    "    lr0=1e-6,\n",
    "    lrf=1e-4,\n",
    "    warmup_epochs=8,\n",
    "    plots=True,\n",
    "    augment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "#     \"ViT-H-14-quickgelu\", pretrained=\"dfn5b\"\n",
    "# )\n",
    "# tokenizer = open_clip.get_tokenizer(\"ViT-H-14-quickgelu\")\n",
    "hf_repo = \"hf-hub:Interpause/ViT-H-14-quickgelu-dfn5b-til24id\"\n",
    "model, preprocess = open_clip.create_model_from_pretrained(\n",
    "    hf_repo, precision=\"fp16\", jit=True\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer(hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../bus.jpg\"\n",
    "image = preprocess(Image.open(img_path)).unsqueeze(0)\n",
    "text = [\n",
    "    \"diagram\",\n",
    "    \"dog\",\n",
    "    \"cat\",\n",
    "    \"bee\",\n",
    "    \"truck\",\n",
    "    \"bus\",\n",
    "    \"school bus\",\n",
    "    \"white school bus\",\n",
    "    \"black and yellow school bus\",\n",
    "]\n",
    "toks = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "    image_features = model.encode_image(image.cuda())\n",
    "    text_features = model.encode_text(toks.cuda())\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", dict(zip(text, text_probs[0].tolist())))  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "MODEL_ARCH = \"ViT-H-14-quickgelu\"\n",
    "MODEL_BASE = \"/workspaces/til24-cv-trainer/notebooks/archive/artifacts/dfn5b.bin\"\n",
    "MODEL_FT = \"/workspaces/til24-cv-trainer/notebooks/archive/artifacts/v2_e28_fp16.bin\"\n",
    "model1, _ = open_clip.create_model_from_pretrained(\n",
    "    MODEL_ARCH,\n",
    "    pretrained=MODEL_BASE,\n",
    "    precision=\"fp16\",\n",
    "    image_resize_mode=\"longest\",\n",
    "    image_interpolation=\"bicubic\",\n",
    ")\n",
    "model2, _ = open_clip.create_model_from_pretrained(\n",
    "    MODEL_ARCH,\n",
    "    pretrained=MODEL_FT,\n",
    "    precision=\"fp16\",\n",
    "    image_resize_mode=\"longest\",\n",
    "    image_interpolation=\"bicubic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = model1.state_dict()\n",
    "theta1 = model2.state_dict()\n",
    "alpha = 0.6\n",
    "\n",
    "# make sure checkpoints are compatible\n",
    "assert set(theta0.keys()) == set(theta1.keys())\n",
    "\n",
    "# interpolate between checkpoints with mixing coefficient alpha\n",
    "theta = {key: (1 - alpha) * theta0[key] + alpha * theta1[key] for key in theta0.keys()}\n",
    "model1.load_state_dict(theta)\n",
    "import pickle, torch\n",
    "\n",
    "torch.save(\n",
    "    theta,\n",
    "    \"/workspaces/til24-cv-trainer/notebooks/archive/artifacts/wiseft-a0.6.bin\",\n",
    "    pickle_protocol=pickle.HIGHEST_PROTOCOL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(model1.cpu().eval()).save(\n",
    "    \"/workspaces/til24-cv-trainer/notebooks/archive/artifacts/wiseft-a0.6-jit.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
